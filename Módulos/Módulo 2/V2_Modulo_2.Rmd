---
title: 'Introducción al Análisis Exploratorio de Datos (EDA) en R'
subtitle: 'Módulo 2'
fontsize: 8 pt
output:
  beamer_presentation:
    theme: "Berlin"
    fonttheme: "structurebold"
    slide_level: 2
    toc: true
date: "2024-03-16"
header-includes:
- \usepackage{booktabs}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,warning = FALSE, message = FALSE)
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  paste0("\n \\", "tiny","\n\n", x, "\n\n \\normalsize")
})


```

```{r , echo=F, warning=F, message=F}
library(tibble)
library (kableExtra)
library(plyr)
library(tidyverse)
library(reshape2)
```



# Herramientas para la manipulación de datos

## Paquetes en R

Los paquetes en R son colecciones de funciones, datos y documentación cuyo objetivo es  extender las capacidades básicas de R. **CRAN** (The Comprehensive R Archive Network) es una red de servidores que almacenan versiones de R, así como librerías en R que cumplen las políticas del repositorio \textcolor{cyan}{(CRAN, 2022)}.

Para instalar paquetes del repositorio **CRAN**:
  
```{r, echo=TRUE, eval = F}
install.packages(“dplyr”)
```

Después de instalar el paquete, se debe cargar la librería:

```{r, echo=TRUE, eval = F}
library(dplyr)
```

Para encontrar la documentación del paquete:
  
```{r, echo=TRUE, eval = F}
help(dplyr)
```

## Tidyverse

::: columns
::: {.column width="50%"}

\bigskip

\bigskip

\bigskip

**Tidyverse** es un conjunto de librerías en R diseñadas para el análisis de datos (importar, transforma, visualizar y modelar con datos) (Wickham, 2019).

\bigskip

Nos concentraremos en las siguientes librerías:
  
- dplyr
- ggplot2
- forcats*
  
:::
  
::: {.column width="50%"}
![Librerías en Tidyverse](Figuras/Tidyverse.png)
:::
:::
  
## Importar datos
  
El primer paso es definir el directorio de trabajo:
  
```{r, echo=TRUE, eval=F}
setwd("path")
```

Nos concentraremos en funciones para importar los siguientes formatos de datos
\begin{table}[ht]
\centering
\resizebox{11cm}{!}{
  \begin{tabular}{|*{4}{c|}}
  \hline
  \textbf{Formato} & \textbf{Formato específico}  & \textbf{Función} & \textbf{Paquete}  \\
  \hline
  Texto o tabulares & CSV   &  read\_csv()        & readr \\  
  \cline{2-4}
  &      Otros formatos de texto    &  read\_delim()   &        readr              \\ 
  \hline
  Formatos de otros programas           &  Excel         & read\_excel()           &  readxl   \\ 
  \cline{2-4}
  & SPSS   &  read\_sav()         &  haven                   \\  
  \cline{2-4}
  & STATA          &  read\_dta()         & haven                     \\  
  \cline{2-4}
  & SAS          &  read\_sas()         &   haven                   \\
  \hline
  
  Formatos propios de R          &  .rda         & load()           &  base   \\ 
  \cline{2-4}
  & .rds   &  readRDS()         &    base                 \\  
  \hline
  \hline
  \end{tabular}
}
\label{tab:multicol}
\end{table}

## Pipe (%>%)

La tubería de comando o *pipeline* (`%>%`) es una herramienta utilizada para el encadenamiento de funciones. El operador nos permite escribir una secuencia de operaciones

Una secuencia en su **forma estándar** sigue la forma

```{r, echo=TRUE, eval=F}
dataset_2 <- dplyr::filter(dataset, attend > 15 & attend != 20)

```

En **forma encadenada**:

```{r, echo=TRUE, eval=F}
dataset_2 <- dataset %>% dplyr::filter(attend > 15 & attend != 20)
```

El siguiente atajo es útil: ![Caption for the picture.](Figuras/Atajo.jpg){width=110}\

## Dplyr

El paquete **dplyr** proporciona una sintaxis para la manipulación de datos. (El operador `%>%` pertenece a la sintaxis de dplyr). Nos concentraremos en las siguientes funciones:
  
![Algunas funciones en el paquete dplyr](Figuras/Dplyr.png){width=300}

## Resumen por grupo

::: columns
::: {.column width="50%"}

\bigskip

Usando las funciones `summarize()` y `group_by()`, obtenemos un resumen descriptivo de la base de datos diferenciado según una o más variables de control. Por ejemplo:
  
```{r, echo=TRUE, eval=F}
# Resumen general
table_1 <- new_dataset %>% filter(Int_attend == "Group 4")
%>%   summarize(MeanAttend = mean(attend),SdAttend = sd(attend)) 
```

```{r, echo=TRUE, eval=F}
# Resumen diferenciado
table_2 <- new_dataset %>% group_by(Int_attend) %>%
  summarize(MeanAttend = mean(attend), SdAttend = sd(attend))
```

La **Figura 10** muestra el funcionamiento de `summarize()` y `group_by()`.

:::
  
::: {.column width="46%"}
![Caption for the picture.](Figuras/Group_summary.png)
:::
:::
  
## ggplot2
  
El paquete **ggplot2** proporciona un sistema coherente para visualizar datos y crear gráficos. La versatilidad de **ggplot2** radica en el uso de la Gramática de Gráficos (*Grammar of Graphics*).

```{r, eval =F, echo=T}
ggplot(dataset, aes()) + geometría + faceta + opciones
```

donde:
  
\begin{enumerate}

\item{\textit{dataset} es un data frame}

\item{Las características del mapa \textbf{aes()} describe los ejes $(x,y)$, el color exterior (\textbf{color} o \textbf{colour}), el color interior (\textbf{fill}), la forma de los puntos (\textbf{shape}), el tipo de línea (\textbf{linetype}) y el tamaño (\textbf{size})}

\item{Los objetos geométricos (\textbf{geometría}) determinan el tipo de gráfico:
    
  \begin{itemize}
  
  \item{Puntos ($geom\_point$)}
  \item{Líneas ($geom\_lines$)}
  \item{Histogramas ($geom\_histogram$)}
  \item{Boxplot ($geom\_boxplot$)}
  
  \end{itemize}
}

\item{La \textbf{faceta} permite dividir un gráfico en múltiples gráficos de acuerdo con grupos}



\end{enumerate}


# Introducción al EDA

## ¿Qué es el EDA?

::: columns
::: {.column width="60%"}

\bigskip

\bigskip

\bigskip

El Análisis Exploratorio de Datos (EDA, por sus siglas en inglés) proporciona una estrategia robusta para ampliar el entendimiento sobre los datos. El principio general es el siguiente:

\bigskip

#### 

“Es importante comprender lo que podemos hacer antes de aprender a medir lo bien que parece que lo hemos hecho” \textcolor{cyan}{(Tukey, 1977)}.

:::
  
::: {.column width="40%"}
![Tukey (1977). Exploratory Data Analysis](Figuras/Tukey.jpg)
:::
:::


## Utilidad

Mediante métodos gráficos y descriptivos, el EDA permite \textcolor{cyan}{(Carranza, 2021)}:

- Revelar la estructura de los datos

- Determinar las variables relevantes
- Determinar valores atípicos
- Proponer hipótesis
- Proponer estrategias para la modelación

## EDA y CDA
![Análisis Exploratorio de Datos (EDA) y Análisis Confirmatorio de Datos (CDA)](Figuras/EDA_CDA.png){width=350}


# Análisis Exploratorio de Datos (EDA) Univariado

## Base de datos

La base de datos usada es extraída de los microdatos de la \textbf{Gran Encuesta Integrada de Hogares (GEIH)} para diciembre de 2023. El análisis considera las siguientes 13 ciudades y áreas metropolitanas:

::: columns
::: {.column width="30%"}

- Medellín A.M.
- Barranquilla A.M.
- Bogotá
- Cartagena
- Manizales A.M.


:::
  
::: {.column width="30%"}

- Monteria
- Villavicencio
- Pasto
- Cucuta A.M.
- Pereira A.M.

:::

::: {.column width="30%"}

- Pereira A.M.
- Bucaramanga A.M.
- Ibague 
- Cali A.M
:::
:::

\bigskip

La información es extraída de dos módulos de la GEIH:

- **Ocupados** (horas trabajadas , ingreso laboral, actividad económica, etc.)
- **Características generales, seguridad social en salud y educación** (edad, sexo, nivel de educación, etc.)


## Base de datos (cont.)

Para importar la base de datos (.xlsx),
```{r, echo=T, eval=F}
library(readxl)
dataset <- readxl::read_excel("Datos/Formatos/geih_dataset.xlsx")
```
La siguiente tabla muestra un resumen de la base de datos:
\begin{table}[ht]
\centering
\resizebox{5.5cm}{!}{
  \begin{tabular}{lll}
  \hline
  \textbf{Variable} & \textbf{Clase}  & \textbf{Descripción}   \\
  \hline
  area & Factor & Área metropolitana \\ 
        dpto & Factor & Departamento \\ 
        sexo & Factor & Sexo al nacer \\ 
        parent & Factor & Parentesco con el jefe o jefa del hogar \\ 
        edad & Numérica & Años cumplidos \\
        edu & Factor & Mayor nivel educativo alcanzado \\ 
        ingreso & Numérica & Ingreso laboral \\ 
        horas\_semana & Numérica & Horas trabajadas normalmente a la semana \\ 
        cotiza & Factor & ¿Cotiza a un fondo de pensiones? \\ 
        lugar & Factor & Lugar principal de trabajo \\ 
        meses & Numérica & ¿Cuántos meses trabajó en los últimos 12 meses? \\ 
        rama\_4 & Cadena & Rama de actividad CIIU REV 4 (4 dígitos) \\ 
        rama\_2 & Cadena & Rama de actividad CIIU REV 4 (4 dígitos) \\ 
        posic & Factor & Posición laboral \\ 
        fondo & Factor & ¿A cuál fondo cotiza? \\ 
        cambiar & Factor & ¿Desea cambiar su trabajo? \\ 
        estable & Factor & ¿Considera que su empleo es estable? \\ 
        t\_actual & Numérica & ¿Cuánto tiempo lleva en su empleo actual? \\ 
        t\_viaje & Numérica & Tiempo de desplazamiento al trabajo \\ 
        mas\_h & Factor & ¿Quiere trabajar más horas? \\ 
        medio & Factor & Medio de transporte \\ 
        sintrab & Factor & ¿Si no tuviera trabajo, de dónde obtendría sus recursos? \\ 
        n\_comp & Factor & ¿Cuántas personas tiene la empresa donde trabajo? \\ 
        srl & Factor & ¿Afiliación a ARL? \\ 
        caja & Factor & ¿Afiliación a caja de compensación familiar? \\ 
        actividad & Factor & Actividad económica recodificada \\ 
        cotiza\_fondo & Factor & Fondo de pensiones recodificado \\ 
        factor\_exp & Numérica & Factor de expansión \\ \hline\hline
  \end{tabular}
}
\label{tab:multicol}
\end{table}

## Datos cualitativos:


### Resumen de datos cualitativos

Considérese las siguientes variables cualitativas previamente recodificadas:

::: columns
::: {.column width="30%"}

\bigskip

- La función `forcats::fct_lump_n()`es usada para agregar las categoría en "otros".

- Las gráficas muestran las 10 categorías más frecuentes.

- La función `ggplot2::facet_wrap` es usada para obtener los gráficos múltiples

:::
  
::: {.column width="70%"}

```{r, echo=F, eval = T}
plot_cat <- readRDS("C:/Users/PC/Desktop/Curso_EDA_2024_I/Módulos/Módulo 2/Figuras/plot_cat.rds")
```


```{r, echo=F, eval = T, fig.height=10, out.height="0.8\\textheight", fig.align="center"}
plot_cat
```
:::
:::

## Resumen de datos cualitativos (cont.):

Idéntica información puede ser representada mediante el siguiente resumen:

```{r, echo=F, eval = T}
count_prop <- tibble(readRDS("C:/Users/PC/Desktop/Curso_EDA_2024_I/Módulos/Módulo 2/Figuras/count_prop.rds"))

count_prop <- count_prop %>% dplyr::filter(!is.na(value))
count_prop <- count_prop %>% dplyr::filter(name %in% c("actividad",
                                                       "edu",
                                                       "cotiza_fondo"))
  
count_prop$name <-  revalue(factor(count_prop$name), 
                                  c("actividad"="Actividad económica", "edu" = "Educación",
                                    "cotiza_fondo" = "Fondo de pensiones"))  
#count_prop$value = substr(count_prop$value, 1, 35)
count_prop$Share = round(count_prop$Share, 2)
```

\centering
\resizebox{11cm}{!}{ 

```{r, echo=F, eval = T}
kable(count_prop[,2:ncol(count_prop)], format = "latex", booktabs = T,
      col.names = c(" ", "N", "Proporción (%)")) %>% 
  pack_rows(index = table(count_prop$name))
```

}

## Resumen de datos cualitativos (cont.):

Un panel de **gráficos circulares** es útil para presentar un resumen sobre las variables categóricas con un número menor de niveles. Considérese las siguientes variables


```{r, echo=F, eval = T}
plot_dic <- readRDS("C:/Users/PC/Desktop/Curso_EDA_2024_I/Módulos/Módulo 2/Figuras/plot_dic.rds")
```
```{r, echo=F, eval = T, fig.height=5, fig.align="center"}
plot_dic
```


## Datos cuantitativos


::: columns
::: {.column width="30%"}

\bigskip

####

El **Histograma** es una representación gráfica de los datos que muestra la frecuencia de los casos (valores) en categorías de datos (véase la tabla inferior).


:::
  
::: {.column width="70%"}

```{r, echo=FALSE, eval = T, warning=F}
dataset <- readxl::read_excel("geih_dataset.xlsx")
ggplot(data = dataset) +
  geom_histogram(mapping = aes(x = ingreso/1000, col = ingreso),
                 fill = "lightskyblue", col = "black",
                 binwidth = 400) + xlim(c(0, 15000))  + theme_bw()
```


:::
:::

```{r, echo=FALSE, eval = T}
count = dataset %>% dplyr::count(cut_width(ingreso/1000, 1000, boundary = 0, dig.lab = 6),
                          name = "n") %>% t()
colnames(count) = count[1,]
count2 = as.data.frame(count[2,1:8]) %>% t()


kable(count2, col.names = NA, row.names = F, format = "latex", booktabs = T)
```

## Histograma


```{r, echo = F, eval = T, , warning=F, prompt=FALSE, results = FALSE}
hist_ds <- dataset[c("id", "edad",
                     "horas_semana", "t_actual",
                     "t_viaje")]

hist_melt <- melt(hist_ds)

figure_3 <- ggplot(data = hist_melt, aes(x = value,
                                         fill = variable)) + geom_histogram(bins = 15,
                                                                            col = "black") + 
  facet_wrap(~variable, scales = "free",
             labeller = labeller(variable = c(`edad` = "Edad",
                                              `horas_semana` = "Horas trabajadas (semana)",
                                              `t_actual` = "Tiempo (empleo actual)",
                                              `t_viaje` = "Tiempo de viaje"))) + guides(fill = "none") + theme_bw() +
  labs(x = " ")
figure_3
```



## PDF y ECDF

La **función de densidad empírica** (PDF) $f(x)$ y la **función de distribución acumulada empírica** (ECDF) $F(x)$ son obtenidas mediante las funciones `density()` y  `ecdf()` del paquete `Stats`.


::: columns
::: {.column width="50%"}

```{r, echo=FALSE, eval = T}
ggplot(data = dataset, aes(x = edad)) + 
  geom_density(color = "black",
               alpha = 0.2, fill = "gray45") +
  theme_bw() + labs(x = "Edad", y = "Density",
                    title = "Edad (años cumplidos)")
```


:::
  
::: {.column width="50%"}

```{r, echo=FALSE, eval = T}
ggplot(data = dataset,
       aes(x = edad)) + stat_ecdf(geom = "step",
                                  color = "gray45") +
  theme_bw() + labs(x = "Edad", y = "ECDF",
                    title = "Edad (años cumplidos)")
```

:::
:::

La función de densidad $f(x)$ satisface que $\int_{a}^{b}f(x)dx = P[a \leq X \leq b]$, donde $P[a \leq X \leq b]$ significa la probabilidad de que X se encuentre en el intervalo $a$ a $b$. Por definición, $F(x) = P(X \leq x)$, es decir, expresa la probabilidad de que X no sea mayor que el valor de x.

## Función de Distribución Acumulada Empírica

Un ejercicio análogo es implementado para las variables cuantitativas restantes.


```{r, echo=FALSE, eval = T, warning=F, fig.height=5, fig.align="center"}
hist_ds <- dataset[c("id", "edad", "ingreso",
                     "horas_semana", "t_actual",
                     "t_viaje")]
hist_ds$ingreso <- hist_ds$ingreso/1000

hist_melt <- melt(hist_ds)

figure_4 <- ggplot(data = hist_melt, aes(x = value,
                                         fill = variable)) + stat_ecdf(geom = "step",
                                                                       color = "gray45") + 
  facet_wrap(~variable, scales = "free",
             labeller = labeller(variable = c(`edad` = "Edad",
                                              `horas_semana` = "Horas trabajadas (semana)",
                                              `t_actual` = "Tiempo (empleo actual)",
                                              `t_viaje` = "Tiempo de viaje",
                                              `ingreso`= "Ingreso laboral"))) + guides(fill = "none") + theme_bw() +
  labs(x = " ")
figure_4
```


## Diagrama de caja

#### 
Un **diagrama de caja** es un resumen gráfico de los datos con base en la los cuartiles Q1 y Q3, la mediana y el rango intercuartílico (RIC). El siguiente diagrama muestra su elaboración:

![Construcción de un diagrama de caja](Figuras/boxplot_completo.png){width=250}

## Un resumen de datos cuantitativos usando StatDA()

La librería `StatDA()` proporciona una utilidad para representar la distribución y los principales elementos descriptivos de las variables continuas. 

```{r, echo=T, eval = F}
library(StatDA)
library(moments)

me = mean(dataset$edad)
sd = sd(dataset$edad)

StatDA::edaplot(dataset$edad, scatter=TRUE, H.freq=FALSE, box=TRUE, 
                H.breaks=seq(0,100, by = 4),H.col="lightgray", H.border=TRUE, H.labels=FALSE,
                S.pch=1, S.col="blue", S.cex=0.5, D.lwd=2, D.lty=1, D.plot=FALSE,
                P.xlim=c(1, 91), P.cex.lab =1.2, P.log=FALSE, P.main="Histograma,
                función de densidad, gráfico de dispersión y diagrama de caja de la edad",
                P.xlab="Edad (años)", P.plot=TRUE,
                P.ylab="Densidad",
                B.pch=1,B.cex=0.5, B.col="red")
lines(density(dataset$edad), lwd=2, col='blue')
curve(dnorm(x, mean=me, sd=sd), from=0, to=100, add=T,
      col='red', lwd=3)
leg.txt <- c(paste0("Min. = ", round(min(dataset$edad),4)),
             paste0("Max. = ", round(max(dataset$edad),4)),
             paste0("Mean = ", round(mean(dataset$edad),4)),
             paste0("Mediana = ", round(median(dataset$edad),4)),
             paste0("SD = ", round(sd(dataset$edad),4)),
             paste0("Kurtosis = ", round(kurtosis(dataset$edad),4)),
             paste0("Skewness = ", round(skewness(dataset$edad),4)))
legend (x=-3, y=0.028, bty="n", leg.txt)
```

## Resumen de datos cuantitativos (cont.)

```{r, echo=FALSE, eval = T, warning=F, message=F}
library(StatDA)
library(moments)

me = mean(dataset$edad)
sd = sd(dataset$edad)

StatDA::edaplot(dataset$edad, scatter=TRUE, H.freq=FALSE, box=TRUE, 
                H.breaks=seq(0,100, by = 4), H.col="lightgray", H.border=TRUE, H.labels=FALSE,
                S.pch=1, S.col="blue", S.cex=0.5,D.lwd=2, D.lty=1, D.plot=FALSE,
                P.xlim=c(1, 91), P.cex.lab =1.2, P.log=FALSE, P.main="Histograma,
                función de densidad, gráfico de dispersión y diagrama de caja de la edad", P.plot=TRUE,
                P.ylab="Density",
                B.pch=1,B.cex=0.5, B.col="red")
lines(density(dataset$edad), lwd=2, col='blue')
curve(dnorm(x, mean=me, sd=sd), from=0, to=100, add=T,
      col='red', lwd=3)
leg.txt <- c(paste0("Min. = ", round(min(dataset$edad),4)), paste0("Max. = ", round(max(dataset$edad),4)),
             paste0("Mean = ", round(mean(dataset$edad),4)), paste0("Median = ", round(median(dataset$edad),4)),
             paste0("SD = ", round(sd(dataset$edad),4)), paste0("Kurtosis = ", round(kurtosis(dataset$edad),4)), paste0("Skewness = ", round(skewness(dataset$edad),4)))
legend (x=-3, y=0.028, bty="n", leg.txt)
```



## Valores atípicos

La perspectiva univariante selecciona como **valores atípicos** u **outliers** aquellas observaciones que caen fuera de los rangos de la distribución. Un valor atípico se puede producir por alguna de las siguientes cuatro causas \textcolor{cyan}{(Aldás y Uriel, 2017)}:

- **Errores en los datos**: errores en la recolección o introducción de los datos.

- **Erorres voluntarios**: errores intencionados en la respuesta del entrevistado.

- **Errores de muestreo**: errores que son el resultado de introducir en la muestra a individuos pertenecientes a una población distinta a la **población objetivo**.

- **Outliers legítimos**: caso de la población objetivo que, por la variabilidad de las muestras, difiere del resto de casos.


## Detección univariada de valores atípicos

Considérese las siguientes alternativas para la detección univariante de *outliers*:

#### Criterio intercuartílico

$$
x^{*} \in [q_{0.25} - 1.5IQR, q_{0.75} + 1.5IQR]
$$

#### Criterio de valores estandarizados (Hair et al., 2014)

- Para muestras pequeñas ($n < 80$), $x^{*}$ tiene valores estándar de 2.5 o superiores.

- Para muestras mayores ($n \geq 80$), $x^{*}$ tiene valores estándar de 3-4 o superiores.

#### Test de Grubbs

El **Test de Grubbs** supone la normalidad de la distribución \textcolor{cyan}{(Grubbs, 1969; Stefansky, 1971)}. La hipótesis nula (no hay *outliers*) se rechaza si

$$
G > \frac{n-1}{n}\sqrt{\frac{t^{2}_{(\alpha/2n, n-2)}}{n-2+t^{2}_{(\alpha/2n, n-2)}}}
$$

donde $G = {max |x_{i} - \bar{x}|}/{\sigma}$

## Criterio intercuartílico

```{r, echo =F, eval = T, results = FALSE}
# El mismo resultado se puede obtener manualmente
Q1 <- quantile(dataset$ingreso, .25, na.rm = T)    # Cálculo de q0.25
Q3 <- quantile(dataset$ingreso, .75, na.rm = T)    # Cálculo de q0.75
IQR <- IQR(dataset$ingreso, na.rm = T)             # Cálculo del IQR (Q3-Q1)

outliers1 <- dataset %>% filter(ingreso<(Q1 - 1.5*IQR) | ingreso>(Q3 + 1.5*IQR))

# Identificación de los outliers en la gráfica
plot(dataset$ingreso, type='p',
     col=ifelse(dataset$ingreso %in% outliers1$ingreso, "red", "black"),
     pch = ifelse(dataset$ingreso %in% outliers1$ingreso, 17, 1),
     ylim = c(-5000000, 10000000),
     ylab  = "Ingreso laboral")
```


## Criterio de valores estandarizados

Considérese la detección de valores atípicos mediante los siguientes valores estandarizados: $x^{*}$ es un valor atípico cuando $z^{*} \geq 2.5$ (**figura A**); y $x^{*}$ es un valor atípico cuando $z^{*} \geq 4$ (**figura B**).

::: columns
::: {.column width="50%"}

```{r, eval = T, echo=FALSE,message=F, warning=F, prompt=FALSE, results = FALSE}
f_z <- function(x){
  z <- (x-mean(x, na.rm = T))/sd(x, na.rm = T)
  return(z)
}

# Definición del data frame con la variable estandarizada
z <- data.frame(id = seq(1, nrow(dataset), by = 1),
                x = dataset$ingreso,
                z = f_z(dataset$ingreso))

# Seguimos el criterio según el cual z > 4, en muestras grandes,
# es considerado un valor atípico
outliers1 <- z  %>% filter(abs(z) >= 2.5)
outliers2 <- z  %>% filter(abs(z) >= 4)

plot(dataset$ingreso, type='p',
     col=ifelse(dataset$ingreso %in% outliers1$x, "red", "black"),
     pch = ifelse(dataset$ingreso %in% outliers1$x, 17, 1))
abline(h = min(outliers1$x), col="blue", lwd=3, lty=2)

```


:::
  
::: {.column width="50%"}

```{r, eval = T, echo=FALSE,message=F, warning=F, prompt=FALSE, results = FALSE}
plot(dataset$ingreso, type='p',
     col=ifelse(dataset$ingreso %in% outliers2$x, "red", "black"),
     pch = ifelse(dataset$ingreso %in% outliers2$x, 17, 1))
abline(h = min(outliers2$x), col="blue", lwd=3, lty=2)
```


:::
:::


## Resumen de la variable continua con y sin outliers



```{r, eval = T, echo=FALSE,message=F, warning=F, prompt=FALSE, results = FALSE}
# Regresemos al caso de los ingresos
# Considérese el criterio de Hair et al. (2019)

# Definición del data frame con la variable estandarizada
z <- data.frame(x = dataset$ingreso,
                z = f_z(dataset$ingreso))

# Seguimos el criterio según el cual z > 4, en muestras grandes,
# es considerado un valor atípico
outliers2 <- z  %>% filter(abs(z) > 4)

# Eliminar valores NA
# El tratamiento de los valores NA y su estudio
# corresponden al tercer módulo del curso
dataset <- dataset %>% filter(!is.na(ingreso))

# Creación de base de datos sin outliers
dataset_no <- dataset %>% filter(!ingreso %in% outliers2$x)

# Comparación entre ambas distribuciones


par(mfrow = c(1,2))
StatDA::edaplot(dataset$ingreso/1000, scatter=TRUE, H.freq=FALSE, box=TRUE, 
                H.breaks=seq(0,90000, by = 400),
                H.col="lightgray", H.border=TRUE, H.labels=FALSE,
                S.pch=1, S.col="blue", S.cex=0.5,
                D.lwd=2, D.lty=1, D.plot=FALSE,
                P.xlim=c(1, 20000), P.cex.lab =1.2,
                P.log=FALSE, P.main="Resumen con outliers",
                P.xlab="Ingreso laboral (miles $)", P.plot=TRUE,
                P.ylab="Densidad",
                B.pch=1,B.cex=0.5, B.col="red")
lines(density(dataset$ingreso/1000), lwd=2, col='blue')
curve(dnorm(x, mean=mean(dataset$ingreso/1000, na.rm = T),
            sd=sd(dataset$ingreso/1000, na.rm = T)), from=0, to=90000, add=T,
      col='red', lwd=3)
leg.txt <- c(paste0("Min. = ", round(min(dataset$ingreso/1000),4)),
             paste0("Max. = ", round(max(dataset$ingreso/1000),4)),
             paste0("Mean = ", round(mean(dataset$ingreso/1000),4)),
             paste0("Median = ", round(median(dataset$ingreso/1000),4)),
             paste0("Std. dev. = ", round(sd(dataset$ingreso/1000),4)),
             paste0("Kurtosis = ", round(kurtosis(dataset$ingreso/1000),4)),
             paste0("Skewness = ", round(skewness(dataset$ingreso/1000),4)))
legend (x= 8000, y=0.0008, bty="n", leg.txt)

StatDA::edaplot(dataset_no$ingreso/1000, scatter=TRUE, H.freq=FALSE, box=TRUE, 
                H.breaks=seq(0,90000, by = 400),
                H.col="lightgray", H.border=TRUE, H.labels=FALSE,
                S.pch=1, S.col="blue", S.cex=0.5,
                D.lwd=2, D.lty=1, D.plot=FALSE,
                P.xlim=c(1, 20000), P.cex.lab =1.2,
                P.log=FALSE, P.main="Resumen sin outliers",
                P.xlab="Ingreso laboral (miles $)", P.plot=TRUE,
                P.ylab="Density",
                B.pch=1,B.cex=0.5, B.col="red")
lines(density(dataset_no$ingreso/1000), lwd=2, col='blue')
curve(dnorm(x, mean=mean(dataset_no$ingreso/1000, na.rm = T),
            sd=sd(dataset_no$ingreso/1000, na.rm = T)), from=0, to=90000, add=T,
      col='red', lwd=3)
leg.txt <- c(paste0("Min. = ", round(min(dataset_no$ingreso/1000),4)),
             paste0("Max. = ", round(max(dataset_no$ingreso/1000),4)),
             paste0("Mean = ", round(mean(dataset_no$ingreso/1000),4)),
             paste0("Median = ", round(median(dataset_no$ingreso/1000),4)),
             paste0("Std. dev. = ", round(sd(dataset_no$ingreso/1000),4)),
             paste0("Kurtosis = ", round(kurtosis(dataset_no$ingreso/1000),4)),
             paste0("Skewness = ", round(skewness(dataset_no$ingreso/1000),4)))
legend (x=8000, y=0.0008, bty="n", leg.txt)

```





## Alternativas

Algunas alternativas para el tratamiento de valores atípicos son las siguientes:

1. Eliminación de los valores atípicos para garantizar estimaciones correctas sobre la mayoría de la población \textcolor{cyan}{(Judd et al., 2009)}.

2. Suavizar la influencia de los valores atípicos mediante el uso de transformaciones (raíces o logaritmos) para reducir su rango \textcolor{cyan}{(Hamilton, 1992)}.

3. Análisis estadístico **robusto**.

#### Desventajas

- Perdida de información

- No todas las transformaciones conservan el sentido teórico de la escala original.


## Transformaciones para reducir su rango

En lo sucesivo, consideramos el efecto de una transformación logarítmica. Nótese que la transformación logra **reducir el rango** y suavizar, en consecuencia, la influencia de los valores atípicos. (La línea azul indica el umbral a partir del cual un valor es considerado atípico según el **criterio de valores estandarizados**).

```{r, echo=FALSE, warning=FALSE, fig.align='center', fig.height=5}

# Una práctica común sugiere que, cuando la interpretación de la variable
# no está enteramente sujeta a la escala, una transformación puede
# funcionar para suavizar la influencia de outliers

# Outliers para log(x)
z <- data.frame(x = log(dataset$ingreso),
                z = f_z(log(dataset$ingreso)))

# Seguimos el criterio según el cual z > 4, en muestras grandes,
# es considerado un valor atípico
outliers3 <- z  %>% filter(abs(z) > 3)

plot(log(dataset$ingreso), type='p',
     col=ifelse(log(dataset$ingreso) %in% outliers3$x, "red", "black"),
     pch = ifelse(log(dataset$ingreso) %in% outliers3$x, 17, 1))
abline(h = min(outliers3$x), col="blue", lwd=3, lty=2)

```


## Transformaciones


```{r, echo=FALSE, warning=FALSE}

# Considérese la transformación logarítmica
dataset$log_ingreso <- log(dataset$ingreso)

dataset_no <- dataset %>% filter(!log_ingreso %in% outliers3$x)

# Comparación entre ambas distribuciones
par(mfrow = c(1,2))
StatDA::edaplot(dataset$log_ingreso, scatter=TRUE, H.freq=FALSE, box=TRUE, 
                H.breaks=seq(12,20, by = 0.1),
                H.col="lightgray", H.border=TRUE, H.labels=FALSE,
                S.pch=1, S.col="blue", S.cex=0.5,
                D.lwd=2, D.lty=1, D.plot=FALSE,
                P.xlim=c(12, 20), P.cex.lab =1.2,
                P.log=FALSE, P.main="Resumen con outliers",
                P.xlab="Log Ingreso laboral", P.plot=TRUE,
                P.ylab="Density",
                B.pch=1,B.cex=0.5, B.col="red")
lines(density(dataset$log_ingreso), lwd=2, col='blue')
curve(dnorm(x, mean=mean(dataset$log_ingreso, na.rm = T),
            sd=sd(dataset$log_ingreso, na.rm = T)), from=12, to=20, add=T,
      col='red', lwd=3)
leg.txt <- c(paste0("Min. = ", round(min(dataset$log_ingreso),4)),
             paste0("Max. = ", round(max(dataset$log_ingreso),4)),
             paste0("Mean = ", round(mean(dataset$log_ingreso),4)),
             paste0("Median = ", round(median(dataset$log_ingreso),4)),
             paste0("Std. dev. = ", round(sd(dataset$log_ingreso),4)),
             paste0("Kurtosis = ", round(kurtosis(dataset$log_ingreso),4)),
             paste0("Skewness = ", round(skewness(dataset$log_ingreso),4)))
legend (x= 15, y=2, bty="n", leg.txt)

StatDA::edaplot(dataset_no$log_ingreso, scatter=TRUE, H.freq=FALSE, box=TRUE, 
                H.breaks=seq(12,18, by = 0.1),
                H.col="lightgray", H.border=TRUE, H.labels=FALSE,
                S.pch=1, S.col="blue", S.cex=0.5,
                D.lwd=2, D.lty=1, D.plot=FALSE,
                P.xlim=c(12, 18), P.cex.lab =1.2,
                P.log=FALSE, P.main="Resumen sin outliers",
                P.xlab="Log Ingreso laboral", P.plot=TRUE,
                P.ylab="Density",
                B.pch=1,B.cex=0.5, B.col="red")
lines(density(dataset_no$log_ingreso), lwd=2, col='blue')
curve(dnorm(x, mean=mean(dataset_no$log_ingreso, na.rm = T),
            sd=sd(dataset_no$log_ingreso, na.rm = T)), from=12, to=18, add=T,
      col='red', lwd=3)
leg.txt <- c(paste0("Min. = ", round(min(dataset_no$log_ingreso),4)),
             paste0("Max. = ", round(max(dataset_no$log_ingreso),4)),
             paste0("Mean = ", round(mean(dataset_no$log_ingreso),4)),
             paste0("Median = ", round(median(dataset_no$log_ingreso),4)),
             paste0("Std. dev. = ", round(sd(dataset_no$log_ingreso),4)),
             paste0("Kurtosis = ", round(kurtosis(dataset_no$log_ingreso),4)),
             paste0("Skewness = ", round(skewness(dataset_no$log_ingreso),4)))
legend (x= 15, y=2, bty="n", leg.txt)

```

# Recursos alternativos


- La librería `swirl` proporciona un tutorial sobre elementos básicos en R

```{r, echo=TRUE, eval=FALSE}
install.packages(“swirl”)
library (swirl)
swirl()
```



- Data wrangling with dplyr and tidyr (Cheat Sheet): [\textcolor{cyan}{Recurso 1.2}](https://github.com/sergiobarona03/Curso_EDA_2024_I/blob/main/Recursos%20alternativos/M%C3%B3dulo%201/Recurso12.pdf)

- Visualización de datos usando ggplot2 (Guía Rápida): [\textcolor{cyan}{Recurso 1.3}](https://github.com/sergiobarona03/Curso_EDA_2024_I/blob/main/Recursos%20alternativos/M%C3%B3dulo%201/Recurso13.pdf)

- Factors with forcats (Cheat Sheet): [\textcolor{cyan}{Recurso 1.4}](https://github.com/sergiobarona03/Curso_EDA_2024_I/blob/main/Recursos%20alternativos/M%C3%B3dulo%201/Recurso14.pdf)
  

  
# Bibliografía de consulta
  
\begin{itemize}
  
\item{Wickham, H. (2016) GGplot2. Elegant Graphics for Data Analysis. Springer}
  
\item{Grolemund, G. (2014). Hands-On Programming with R. O’Reilly Media: Sebastopol, CA.}

\item{Schutt, R. \& O’Neil, C. (2014). Doing Data Science. O’Reilly Media: Sebastopol, CA.}
  
\item{Wickham \& Grolemund, G. (2016). R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly Media: Sebastopol, CA.}

\item{Aldás J. \& Uriel, E. (2017). Análisis multivariante aplicado con R. Madrid: Ediciones Paraninfo}
  
\end{itemize}


